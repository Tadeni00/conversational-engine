Conversational Negotiation Engine
ğŸ“– Project Overview

The Conversational Negotiation Engine is a multilingual, context-aware negotiation system designed to simulate human-like bargaining and decision-making across multiple languages. It integrates language detection, LLM-powered negotiation phrasing, and real-time decision orchestration into a containerized microservices architecture.

This project demonstrates how to build a negotiation service that can:

Detect language of input dynamically

Respond with contextually appropriate negotiation replies

Handle multiple languages including English (EN), Nigerian Pidgin (PCM), Yoruba (YO), Hausa (HA), and Igbo (IG)

Support remote Large Language Model (LLM) providers

Be deployed using Docker and Docker Compose

ğŸš€ Architecture

The system is composed of three main services:

1. Redis

Acts as a caching and state management layer.

Stores conversation context and pricing history.

Exposes standard Redis ports for inter-service communication.

2. Language Detection Service

Detects the language of a given negotiation request.

Returns normalized language codes and confidence scores.

URL: http://language-detection:8000/detect

3. Negotiation Service

Handles core negotiation logic.

Uses LLM to generate negotiation responses based on context.

Supports multiple languages with customized templates and overrides.

Price determination logic is configurable via environment variables.

4. Conversation Orchestrator

Coordinates language detection and negotiation services.

Acts as the entry point for conversation requests.

Integrates with Redis for context persistence.

ğŸ›  Technology Stack

Python 3.12

FastAPI for HTTP APIs

Uvicorn as ASGI server

Redis for caching and state management

Docker & Docker Compose for container orchestration

LLM Providers: GROQ, OpenAI, Hugging Face (configurable)

Languages Supported: EN, PCM, YO, HA, IG

ğŸ“‚ Project Structure
.
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pytest_cache
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ CACHEDIR.TAG
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ v
â”‚       â””â”€â”€ cache
â”‚           â”œâ”€â”€ lastfailed
â”‚           â””â”€â”€ nodeids
â”œâ”€â”€ .vscode
â”‚   â””â”€â”€ settings.json
â”œâ”€â”€ README
â”œâ”€â”€ cleanup.sh
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ models.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scafold.py
â”œâ”€â”€ secrets
â”‚   â”œâ”€â”€ groq_api_key
â”‚   â”œâ”€â”€ hf_token
â”‚   â”œâ”€â”€ llm_api_key
â”‚   â””â”€â”€ openai_api_key
â”œâ”€â”€ services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conversation_engine
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”‚   â””â”€â”€ __init__.cpython-312.pyc
â”‚   â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”‚   â”œâ”€â”€ intent_samples.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ product_catalog.json
â”‚   â”‚   â”‚   â””â”€â”€ prompts_sample.jsonl
â”‚   â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”‚   â”œâ”€â”€ intent_model.pkl
â”‚   â”‚   â”‚   â””â”€â”€ lid.176.ftz
â”‚   â”‚   â”œâ”€â”€ scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ test_language-detection.py
â”‚   â”‚   â”‚   â”œâ”€â”€ train_intent_model.py
â”‚   â”‚   â”‚   â””â”€â”€ train_lora_13b.py
â”‚   â”‚   â””â”€â”€ src
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ core
â”‚   â”‚       â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.cpython-312.pyc
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ app.cpython-312.pyc
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ conversation_engine.cpython-312.pyc
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ intent_classifier.cpython-312.pyc
â”‚   â”‚       â”‚   â”œâ”€â”€ amount_parser.py
â”‚   â”‚       â”‚   â”œâ”€â”€ app.py
â”‚   â”‚       â”‚   â”œâ”€â”€ audit.py
â”‚   â”‚       â”‚   â”œâ”€â”€ conversation_engine.py
â”‚   â”‚       â”‚   â”œâ”€â”€ cultural_intelligence.py
â”‚   â”‚       â”‚   â”œâ”€â”€ decision_model.py
â”‚   â”‚       â”‚   â”œâ”€â”€ intent_classifier.py
â”‚   â”‚       â”‚   â”œâ”€â”€ language_detection.py
â”‚   â”‚       â”‚   â”œâ”€â”€ models.py
â”‚   â”‚       â”‚   â”œâ”€â”€ price_guard.py
â”‚   â”‚       â”‚   â”œâ”€â”€ product_catalog.py
â”‚   â”‚       â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚       â”‚   â”œâ”€â”€ state_store.py
â”‚   â”‚       â”‚   â””â”€â”€ utils.py
â”‚   â”‚       â””â”€â”€ language_detection.py
â”‚   â””â”€â”€ negotiation_service
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ __pycache__
â”‚       â”‚   â””â”€â”€ app.cpython-312.pyc
â”‚       â”œâ”€â”€ app.py
â”‚       â”œâ”€â”€ app.py.bak
â”‚       â”œâ”€â”€ entrypoint.sh
â”‚       â”œâ”€â”€ llm_phraser.py
â”‚       â””â”€â”€ requirements.txt
â””â”€â”€ tests
    â”œâ”€â”€ __pycache__
    â”‚   â””â”€â”€ test_handle.cpython-312-pytest-8.4.1.pyc
    â”œâ”€â”€ test_audit.py
    â”œâ”€â”€ test_handle.py
    â”œâ”€â”€ test_handle_endpoint.sh
    â”œâ”€â”€ test_intent_classifier.sh
    â””â”€â”€ test_languages.sh


âš™ï¸ Setup Instructions
Prerequisites

Docker â‰¥ 20.x

Docker Compose â‰¥ 2.x

API keys for your LLM provider(s):

GROQ

OpenAI

Hugging Face

Clone the Repository
git clone <repo-url>
cd conversational-engine

Add Secrets

Place your API keys in ./secrets/:

secrets/
â”œâ”€â”€ groq_api_key
â”œâ”€â”€ openai_api_key
â”œâ”€â”€ hf_token
â”œâ”€â”€ llm_api_key

Run the Services
docker compose up --build


This will:

Start Redis

Start Language Detection service on port 8000

Start Negotiation service on port 9000

Start Conversation Orchestrator on port 9100

ğŸ§ª Testing

Run the provided language test script:

tests/test_languages.sh


Expected output:

{
  "action": "COUNTER",
  "price": 7800,
  "confidence": 1.0,
  "strategy_tag": null,
  "meta": null,
  "reply": "<language-specific reply>"
}

ğŸ”§ Configuration

All services support environment variables for configuration. For example:

LLM_MODE â€“ Mode of LLM (LOCAL / REMOTE)

LLM_REMOTE_PROVIDER â€“ Remote LLM provider

LLM_MODEL â€“ LLM model to use

LLM_REMOTE_URL â€“ LLM endpoint

GROQ_API_KEY_FILE, OPENAI_API_KEY_FILE, HF_TOKEN_FILE, LLM_API_KEY_FILE â€“ API key files

These variables are set in docker-compose.yml and overridden via .env.

ğŸŒ Multilingual Support

The system supports dynamic language detection and template-based negotiation. Current supported languages:

English (en)

Nigerian Pidgin (pcm)

Yoruba (yo)

Hausa (ha)

Igbo (ig)

Language-specific replies are defined and overridden in the llm_phraser.py.

ğŸ“Œ Future Improvements

Add more languages and dialect support

Implement persistent conversation state across sessions

Add more sophisticated pricing negotiation strategies

Integrate with external APIs for dynamic product pricing

ğŸ›¡ Security

API keys are stored securely as Docker secrets

Environment variables control LLM access and provider configurations

ğŸ“š References

FastAPI

Redis

Docker Compose

ğŸ“ Author

Tomisin Adeniyi (Tadeni00)
Conversational AI Engineer





docker compose build --no-cache
docker compose up --no-deps
docker compose logs -f language-detection negotiation 

tests/test_languages.sh

